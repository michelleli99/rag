{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LLM Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict, Any\n",
    "from abc import ABC\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define LLM judge output structure \n",
    "class JudgeOutput(TypedDict):\n",
    "    score: int \n",
    "    justification:str \n",
    "\n",
    "# Define superclass \n",
    "class LLMJudge(ABC):\n",
    "    \"\"\"\n",
    "    Base class for LLM judges.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, prompt: str, model: str = \"gpt-5-mini\"):\n",
    "        self.name = name\n",
    "        self.prompt = prompt\n",
    "        self.llm = ChatOpenAI(model=model)\n",
    "    \n",
    "    def format_prompt(self, answer: str, context: str = \"\", question: str = \"\") -> str:\n",
    "        base = f\"Evaluation Task: {self.prompt}\\n\\n\"\n",
    "        if question:\n",
    "            base += f\"User Question:\\n{question}\\n\\n\"\n",
    "        if context:\n",
    "            base += f\"Reference Context:\\n{context}\\n\\n\"\n",
    "        base += f\"Answer to Evaluate:\\n{answer}\\n\\n\"\n",
    "        base += \"Respond with a JSON object: {\\\"score\\\": 1-5, \\\"justification\\\": \\\"string\\\"}\"\n",
    "        return base\n",
    "    \n",
    "    def evaluate(self, answer: str, context: str = \"\", question: str = \"\") -> JudgeOutput:\n",
    "        \"\"\"\n",
    "        Calls LLM \n",
    "        \"\"\"\n",
    "        prompt = self.format_prompt(answer, context, question)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            parsed = json.loads(response.content)\n",
    "            return JudgeOutput(score=parsed[\"score\"], justification=parsed[\"justification\"])\n",
    "        except Exception as e:\n",
    "            return JudgeOutput(score=0, justification=f\"Parsing error: {e}, raw: {response.content}\")\n",
    "\n",
    "# Define custom judges\n",
    "class ContextRelevanceJudge(LLMJudge):\n",
    "    def __init__(self, model=\"gpt-5-mini\"):\n",
    "        super().__init__(\n",
    "            name=\"ContextRelevance\",\n",
    "            prompt=\"\"\"Evaluate how relevant the answer is to the given context. \n",
    "            A score of 5 means the response was completely relevant. The context contains the necessary information to answer the Human query.\n",
    "            A score of 1 means the response was completely irrelevant. The context does not contain information to answer the Human query.\"\"\",\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "class AnswerGroundednessJudge(LLMJudge):\n",
    "    def __init__(self, model=\"gpt-5-mini\"):\n",
    "        super().__init__(\n",
    "            name=\"AnswerGroundedness\",\n",
    "            prompt=\"\"\"Evaluate how well the answer is grounded in the provided context, avoiding hallucinations.\n",
    "            A score of 5 means that the generated response is well-grounded in the context. All statements from the answer are supported in the context.\n",
    "            A score of 1 means that the generated response is not grounded. The statements in the answer are hallucinated and not supported by the context.\"\"\",\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "class AnswerRelevanceJudge(LLMJudge):\n",
    "    def __init__(self, model=\"gpt-5-mini\"):\n",
    "        super().__init__(\n",
    "            name=\"AnswerRelevance\",\n",
    "            prompt=\"\"\"Evaluate how directly the answer addresses the user’s question.\n",
    "            A score of 5 means that the generated response is relevant to the Human query. It answers the Human query.\n",
    "            A score of 1 means that the generated response is irrelevant to the Human query. It is not helpful in answer the query.\"\"\",\n",
    "            model=model\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "judges = [\n",
    "    ContextRelevanceJudge(),\n",
    "    AnswerRelevanceJudge(),\n",
    "    AnswerGroundednessJudge()\n",
    "]\n",
    "\n",
    "answer = \"Mount Rainier is the highest mountain in Washington State.\"\n",
    "context = \"Mount Rainier is a stratovolcano located in the Cascade Range of Washington. It stands at 14,411 feet and is the most glaciated peak in the contiguous United States.\"\n",
    "question = \"Where is Mount Rainier located?\"\n",
    "\n",
    "for judge in judges:\n",
    "    # Print the prompt for each judge\n",
    "    prompt = judge.format_prompt(answer, context, question)\n",
    "    print(f\"\\n--- {judge.name} Prompt ---\\n{prompt}\\n\")\n",
    "\n",
    "    # Run evaluation \n",
    "    result = judge.evaluate(answer, context, question)\n",
    "    print(judge.name, \"→\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
